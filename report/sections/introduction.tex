\documentclass[../main.tex]{subfiles}


\begin{document}

\begin{itemize}
    \item Motivation
    \begin{itemize}
        \item Large Language Models (LLMs) have shown exceptional performance across a range of tasks, yet their complexity renders their inner workings opaque.
        This opacity challenges our ability to understand, trust, and safely deploy these models in real-world applications.
        To rectify this, the overlapping fields of Explainable AI (XAI) and Mechanistic Interpretability (MI) have emerged.
        The former focuses on developing methods to explain the predictions of LLMs, while the latter focuses on understanding the inner workings of LLMs.
        Much of the focus in MI has been on understanding the behaviour of individual neurons, but as demonstrated in \citet{elhage_toy_2022}, the behaviour of individual neurons often doesn't map onto human understandable concepts.
        \citet{bricken_towards_2023} shows a possible way forward by providing the features of SAEs (Sparse Autoencoders) as alternative units of interpretability. 
    \end{itemize}
    \item SotA
    \begin{itemize}
        \item How much should I mention articles that are irrelevant for my own work, but are in the same area?
        \item 
    \end{itemize}
    \item Problem statement
    \begin{itemize}
        \item In this thesis, we attempt to apply the N2G method to the features of SAEs.
        The goals of this are twofold.
        Firstly, this is a test of both methods, since if SAEs truly do provide more interpretable features and N2G truly does provide a useful representation of feature behaviour, we would expect this to be reflected when comparing N2G graphs for individual neurons against those for features. \todo{Info on what comparisons we expect to make.}
        Secondly, it is possible that the results could be useful in their own right for understanding models.
    \end{itemize}
\end{itemize}

\subbib
\end{document}