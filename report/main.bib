
@misc{foote_neuron_2023,
	title = {Neuron to {Graph}: {Interpreting} {Language} {Model} {Neurons} at {Scale}},
	shorttitle = {Neuron to {Graph}},
	url = {http://arxiv.org/abs/2305.19911},
	doi = {10.48550/arXiv.2305.19911},
	abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
	month = may,
	year = {2023},
	note = {arXiv:2305.19911 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\EHVYIWBD\\Foote et al. - 2023 - Neuron to Graph Interpreting Language Model Neuro.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\XLRS23N4\\2305.html:text/html},
}

@misc{garde_deepdecipher_2023,
	title = {{DeepDecipher}: {Accessing} and {Investigating} {Neuron} {Activation} in {Large} {Language} {Models}},
	copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Licence (CC-BY-NC-ND)},
	shorttitle = {{DeepDecipher}},
	url = {http://arxiv.org/abs/2310.01870},
	doi = {10.48550/arXiv.2310.01870},
	abstract = {As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Researchers, engineers, and developers can quickly diagnose issues, audit systems, and advance the field.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Garde, Albert and Kran, Esben and Barez, Fazl},
	month = nov,
	year = {2023},
	note = {arXiv:2310.01870 [cs]},
	keywords = {Computer Science - Machine Learning, 68T50 (Primary) 68T05 (Secondary), I.2.7},
	annote = {Comment: 5 pages (9 total), 1 figure, submitted to NeurIPS 2023 Workshop XAIA},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\3RIPKI9H\\Garde et al. - 2023 - DeepDecipher Accessing and Investigating Neuron A.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\HPGFUTED\\2310.html:text/html},
}

@misc{bloom_open_2024,
	title = {Open {Source} {Sparse} {Autoencoders} for all {Residual} {Stream} {Layers} of {GPT2}-{Small}},
	url = {https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream},
	language = {en},
	urldate = {2024-02-09},
	author = {Bloom, Joseph},
	month = feb,
	year = {2024},
	file = {Snapshot:C\:\\Users\\alber\\Zotero\\storage\\7CSP42FU\\open-source-sparse-autoencoders-for-all-residual-stream.html:text/html},
}

@misc{cunningham_sparse_2023,
	title = {Sparse {Autoencoders} {Find} {Highly} {Interpretable} {Features} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.08600},
	doi = {10.48550/arXiv.2309.08600},
	abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
	month = oct,
	year = {2023},
	note = {arXiv:2309.08600 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 20 pages, 18 figures, 2 tables},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\GGYGXFZ5\\Cunningham et al. - 2023 - Sparse Autoencoders Find Highly Interpretable Feat.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\MPIW2KKM\\2309.html:text/html},
}

@misc{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	doi = {10.48550/arXiv.2002.12327},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue and approaches to compression. We then outline directions for future research.},
	urldate = {2024-02-09},
	publisher = {arXiv},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = nov,
	year = {2020},
	note = {arXiv:2002.12327 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to TACL. Please note that the multilingual BERT section is only available in version 1},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\9H3J65NM\\Rogers et al. - 2020 - A Primer in BERTology What we know about how BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\44E2CYUZ\\2002.html:text/html},
}

@article{elhage_toy_2022,
	title = {Toy {Models} of {Superposition}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
	year = {2022},
	annote = {https://transformer-circuits.pub/2022/toy\_model/index.html},
}

@article{bricken_towards_2023,
	title = {Towards {Monosemanticity}: {Decomposing} {Language} {Models} {With} {Dictionary} {Learning}},
	journal = {Transformer Circuits Thread},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	year = {2023},
	annote = {https://transformer-circuits.pub/2023/monosemantic-features/index.html},
}

@misc{conmy_towards_2023,
	title = {Towards {Automated} {Circuit} {Discovery} for {Mechanistic} {Interpretability}},
	url = {http://arxiv.org/abs/2304.14997},
	doi = {10.48550/arXiv.2304.14997},
	abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
	month = oct,
	year = {2023},
	note = {arXiv:2304.14997 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: NeurIPS 2023 Spotlight},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\2FWBXADC\\Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\MNJ469VS\\2304.html:text/html},
}

@misc{bolukbasi_interpretability_2021,
	title = {An {Interpretability} {Illusion} for {BERT}},
	url = {http://arxiv.org/abs/2104.07143},
	doi = {10.48550/arXiv.2104.07143},
	abstract = {We describe an "interpretability illusion" that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
	urldate = {2024-02-12},
	publisher = {arXiv},
	author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Viégas, Fernanda and Wattenberg, Martin},
	month = apr,
	year = {2021},
	note = {arXiv:2104.07143 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\X3L3IRUW\\Bolukbasi et al. - 2021 - An Interpretability Illusion for BERT.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\GKKVL2M3\\2104.html:text/html},
}

@misc{hao_self-attention_2021,
	title = {Self-{Attention} {Attribution}: {Interpreting} {Information} {Interactions} {Inside} {Transformer}},
	shorttitle = {Self-{Attention} {Attribution}},
	url = {http://arxiv.org/abs/2004.11207},
	doi = {10.48550/arXiv.2004.11207},
	abstract = {The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Hao, Yaru and Dong, Li and Wei, Furu and Xu, Ke},
	month = feb,
	year = {2021},
	note = {arXiv:2004.11207 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: AAAI-2021},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\IBJ5ZJMU\\Hao et al. - 2021 - Self-Attention Attribution Interpreting Informati.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\6UC5SYBS\\2004.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-02-14},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\alber\\Zotero\\storage\\R6C7FF8F\\Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\alber\\Zotero\\storage\\G6B5WF34\\1706.html:text/html},
}

@article{elhage_mathematical_2021,
	title = {A {Mathematical} {Framework} for {Transformer} {Circuits}},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2021},
	annote = {https://transformer-circuits.pub/2021/framework/index.html},
}

@article{olsson_-context_2022,
	title = {In-context {Learning} and {Induction} {Heads}},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
	annote = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
}
