{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from jaxtyping import Float, Int\n",
    "import requests\n",
    "import functools\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "from n2g import NeuronModel, FeatureModel, Tokenizer\n",
    "import thesis\n",
    "\n",
    "importlib.reload(thesis)\n",
    "from thesis import n2g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_large = HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32a4fc993d44b15bee9f94599372795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the cat is smwing me â™¥\\n\\ncat swallowed girl drinkers'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_large.generate(\"the cat is sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19.7922, 19.6165, 19.4624, 18.2938, 17.9073], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eared', 'itten', 'elly', 'ot', 'okin']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = model_large.to_tokens(\"the cat is sm\")\n",
    "top_logits, top_tokens = model_large.run_with_hooks(sample)[0, -1, :].topk(k=5, dim=-1)\n",
    "print(top_logits)\n",
    "model_large.to_str_tokens(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://deepdecipher.org/api/gpt2-small/neuron2graph-search?query=activating:sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[]] * 12\n",
    "for index in r.json()[\"data\"]:\n",
    "    layer = index[\"layer\"]\n",
    "    neuron = index[\"neuron\"]\n",
    "    layers[layer].append(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([20.9554, 20.9068, 20.5438, 19.5182, 18.8950], device='cuda:0',\n",
      "       grad_fn=<TopkBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eared', 'elly', 'itten', 'ot', 'okin']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hooks = []\n",
    "\n",
    "\n",
    "def hook_fn(\n",
    "    indices: Int[Tensor, \" _\"], activation: Float[Tensor, \"batch context neurons_per_layer\"], hook: HookPoint\n",
    ") -> None:\n",
    "    activation[:, -1, indices] = 0.0\n",
    "\n",
    "\n",
    "for layer_index, neurons in enumerate(layers):\n",
    "    indices = torch.tensor(neurons)\n",
    "    hook = functools.partial(hook_fn, indices)\n",
    "    hooks.append((f\"blocks.{layer_index}.mlp.hook_post\", hook))\n",
    "\n",
    "\n",
    "top_logits, top_tokens = model_large.run_with_hooks(sample, fwd_hooks=hooks)[0, -1, :].topk(k=5, dim=-1)\n",
    "print(top_logits)\n",
    "model_large.to_str_tokens(top_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8657842ab84f9fa2500a3bf67c55d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the cat is smothering my ear and my right ear.yes'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_large.generate(\"the cat is sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gelu-1l into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69bf027e0fb14e3f8ef82b309291259a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'the cat is smitten by several sadities. She has not said'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gelu = HookedTransformer.from_pretrained(\"gelu-1l\")\n",
    "model_gelu.generate(\"the cat is sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([',', 'and', 'user', '0', '3', '.', 'glad', 'name', 'queries', 'just', 'humans', 'suggests', 'related', 'ning', 'sm', 'iles', 'family'])\n",
      "dict_keys(['system', 'arlington', 'ole', 'sm', 'dent', 'vector', 'OW', 'th', 'igator'])\n"
     ]
    }
   ],
   "source": [
    "n2g_path = Path(\"outputs/gelu-1l-sae-n2g\")\n",
    "\n",
    "\n",
    "def activates_on(n2g_model: NeuronModel, token: str | list[str]) -> bool:\n",
    "    if isinstance(token, str):\n",
    "        token = [token]\n",
    "    activating = n2g_model.trie_root.children.keys()\n",
    "    return any(t in activating for t in token)\n",
    "\n",
    "\n",
    "sm_n2g_models = [model for model in n2g.iter_models(n2g_path, range(2048, 4096)) if activates_on(model, [\"sm\", \" sm\"])]\n",
    "\n",
    "len(sm_n2g_models)\n",
    "for n2g_model in sm_n2g_models:\n",
    "    print(n2g_model.trie_root.children.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "given string partly should be tokenized to exactly one token",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m root \u001b[38;5;241m=\u001b[39m n2g_model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[0;32m     11\u001b[0m gelu_tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(model_gelu)\n\u001b[1;32m---> 12\u001b[0m rs_model \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgelu_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn2g_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:93\u001b[0m, in \u001b[0;36mFeatureModel.from_model\u001b[1;34m(tokenizer, model)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_model\u001b[39m(tokenizer: Tokenizer, model: NeuronModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     92\u001b[0m     trie_root \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[1;32m---> 93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrie_root\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureModel(RustFeatureModel\u001b[38;5;241m.\u001b[39mfrom_nodes(nodes), tokenizer)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_model\u001b[39m(tokenizer: Tokenizer, model: NeuronModel) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatureModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     92\u001b[0m     trie_root \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrie_root\n\u001b[0;32m     93\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 94\u001b[0m         (_str_token_to_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m trie_root\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     96\u001b[0m     ]\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FeatureModel(RustFeatureModel\u001b[38;5;241m.\u001b[39mfrom_nodes(nodes), tokenizer)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (_str_token_to_pattern_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (_str_token_to_pattern_token(tokenizer, str_token), \u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "    \u001b[1;31m[... skipping similar frames: <listcomp> at line 67 (2 times), _node_rust_to_py at line 66 (2 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:66\u001b[0m, in \u001b[0;36m_node_rust_to_py\u001b[1;34m(tokenizer, node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[1;32m---> 66\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_node_rust_to_py\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mneuron_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_TOKEN\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_node_rust_to_py\u001b[39m(tokenizer: Tokenizer, node: NeuronNode) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RustFeatureModelNode:\n\u001b[0;32m     66\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 67\u001b[0m         (\u001b[43m_str_token_to_pattern_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m, _node_rust_to_py(tokenizer, child))\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m str_token, child \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mchildren\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m!=\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mEND_TOKEN\n\u001b[0;32m     70\u001b[0m     ]\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mis_end:\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m RustFeatureModelNode\u001b[38;5;241m.\u001b[39mfrom_children(children, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mimportance, node\u001b[38;5;241m.\u001b[39mvalue\u001b[38;5;241m.\u001b[39mactivation)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\feature_model.py:55\u001b[0m, in \u001b[0;36m_str_token_to_pattern_token\u001b[1;34m(tokenizer, str_token)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m str_token \u001b[38;5;241m==\u001b[39m neuron_model\u001b[38;5;241m.\u001b[39mIGNORE_TOKEN:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PatternToken\u001b[38;5;241m.\u001b[39mignore()\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PatternToken\u001b[38;5;241m.\u001b[39mregular(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_token\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Documents\\Neuron2Graph\\n2g\\tokenizer.py:22\u001b[0m, in \u001b[0;36mTokenizer.str_to_id\u001b[1;34m(self, str_token)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstr_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, str_token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m     21\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mencode(str_token)\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoding) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgiven string \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstr_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m should be tokenized to exactly one token\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoding[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mAssertionError\u001b[0m: given string partly should be tokenized to exactly one token"
     ]
    }
   ],
   "source": [
    "def print_trie(node, depth: int) -> None:\n",
    "    print(\" \" * depth + \"'\" + node.value.token + \"'\")\n",
    "    for str_token, child in node.children.items():\n",
    "        assert str_token == child.value.token, f\"'{str_token}' != '{node.value.token}'\"\n",
    "        print_trie(child, depth + 1)\n",
    "\n",
    "\n",
    "n2g_model = sm_n2g_models[0]\n",
    "root = n2g_model.trie_root\n",
    "\n",
    "gelu_tokenizer = Tokenizer(model_gelu)\n",
    "rs_model = FeatureModel.from_model(gelu_tokenizer, n2g_model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepdecipher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
